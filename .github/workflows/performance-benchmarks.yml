name: üöÄ Performance Benchmarking

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance benchmarks every night at 2 AM
    - cron: '0 2 * * *'

env:
  BENCHMARK_ITERATIONS: 3
  BENCHMARK_TIMEOUT: 300

jobs:
  performance-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: routeforce_bench
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: üêç Set up Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: üì¶ Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-benchmark-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-benchmark-
            ${{ runner.os }}-pip-

      - name: ‚öôÔ∏è Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark memory-profiler psutil

      - name: üèÉ‚Äç‚ôÇÔ∏è Run Route Optimization Benchmarks
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/routeforce_bench
          REDIS_URL: redis://localhost:6379/0
          FLASK_ENV: benchmarking
        run: |
          python -c "
          import pytest
          import sys
          import os
          
          # Create benchmark test file if it doesn't exist
          benchmark_content = '''
import pytest
import time
import psutil
import os
from app import create_app
from app.algorithms.genetic_algorithm import GeneticAlgorithmOptimizer
from app.algorithms.simulated_annealing import SimulatedAnnealingOptimizer
from app.services.routing_service import RoutingService

@pytest.fixture
def app():
    app = create_app(\"\"testing\"\")
    return app

@pytest.fixture
def routing_service():
    return RoutingService()

def test_genetic_algorithm_performance(benchmark):
    \"\"\"Benchmark genetic algorithm optimization\"\"\"
    optimizer = GeneticAlgorithmOptimizer()
    
    # Sample route data for testing
    sample_data = {
        \"\"locations\"\": [
            {\"\"lat\"\": 40.7128, \"\"lng\"\": -74.0060, \"\"address\"\": \"\"NYC\"\"},
            {\"\"lat\"\": 40.7589, \"\"lng\"\": -73.9851, \"\"address\"\": \"\"Times Square\"\"},
            {\"\"lat\"\": 40.7505, \"\"lng\"\": -73.9934, \"\"address\"\": \"\"Herald Square\"\"},
            {\"\"lat\"\": 40.7614, \"\"lng\"\": -73.9776, \"\"address\"\": \"\"Central Park\"\"},
            {\"\"lat\"\": 40.7282, \"\"lng\"\": -73.7949, \"\"address\"\": \"\"Queens\"\""}
        ]
    }
    
    def run_optimization():
        return optimizer.optimize(sample_data[\"\"locations\"\"])
    
    result = benchmark.pedantic(run_optimization, iterations=${{ env.BENCHMARK_ITERATIONS }}, rounds=1)
    
    # Assert performance expectations
    assert result is not None
    print(f\"\"‚úÖ Genetic Algorithm completed in {benchmark.stats.mean:.4f}s avg\"\")

def test_simulated_annealing_performance(benchmark):
    \"\"\"Benchmark simulated annealing optimization\"\"\"
    optimizer = SimulatedAnnealingOptimizer()
    
    # Sample route data for testing
    sample_data = {
        \"\"locations\"\": [
            {\"\"lat\"\": 40.7128, \"\"lng\"\": -74.0060, \"\"address\"\": \"\"NYC\"\"},
            {\"\"lat\"\": 40.7589, \"\"lng\"\": -73.9851, \"\"address\"\": \"\"Times Square\"\"},
            {\"\"lat\"\": 40.7505, \"\"lng\"\": -73.9934, \"\"address\"\": \"\"Herald Square\"\"},
            {\"\"lat\"\": 40.7614, \"\"lng\"\": -73.9776, \"\"address\"\": \"\"Central Park\"\"},
            {\"\"lat\"\": 40.7282, \"\"lng\"\": -73.7949, \"\"address\"\": \"\"Queens\"\""}
        ]
    }
    
    def run_optimization():
        return optimizer.optimize(sample_data[\"\"locations\"\"])
    
    result = benchmark.pedantic(run_optimization, iterations=${{ env.BENCHMARK_ITERATIONS }}, rounds=1)
    
    # Assert performance expectations
    assert result is not None
    print(f\"\"‚úÖ Simulated Annealing completed in {benchmark.stats.mean:.4f}s avg\"\")

def test_memory_usage():
    \"\"\"Test memory usage during optimization\"\"\"
    process = psutil.Process(os.getpid())
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    
    optimizer = GeneticAlgorithmOptimizer()
    sample_data = {
        \"\"locations\"\": [
            {\"\"lat\"\": 40.7128, \"\"lng\"\": -74.0060, \"\"address\"\": \"\"NYC\"\"},
            {\"\"lat\"\": 40.7589, \"\"lng\"\": -73.9851, \"\"address\"\": \"\"Times Square\"\"},
        ] * 10  # Increase data size
    }
    
    result = optimizer.optimize(sample_data[\"\"locations\"\"])
    
    memory_after = process.memory_info().rss / 1024 / 1024  # MB
    memory_used = memory_after - memory_before
    
    print(f\"\"üìä Memory usage: {memory_used:.2f} MB\"\")
    assert memory_used < 100  # Should use less than 100MB
    assert result is not None
'''
          
          os.makedirs('tests/benchmarks', exist_ok=True)
          with open('tests/benchmarks/test_performance.py', 'w') as f:
              f.write(benchmark_content)
          
          # Run the benchmarks
          exit_code = pytest.main([
              'tests/benchmarks/test_performance.py',
              '--benchmark-only',
              '--benchmark-sort=mean',
              '--benchmark-json=benchmark_results.json',
              '-v'
          ])
          
          sys.exit(0 if exit_code == 0 else exit_code)
          "

      - name: üìä Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            benchmark_results.json
          retention-days: 30

      - name: üìà Performance Report
        if: always()
        run: |
          echo "## üéØ Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          
          if [ -f benchmark_results.json ]; then
            python -c "
            import json
            import sys
            
            try:
                with open('benchmark_results.json', 'r') as f:
                    data = json.load(f)
                
                print('## üìä Benchmark Summary', file=sys.stderr)
                print('| Algorithm | Mean Time (s) | Min Time (s) | Max Time (s) |', file=sys.stderr)
                print('|-----------|---------------|--------------|--------------|', file=sys.stderr)
                
                for benchmark in data.get('benchmarks', []):
                    name = benchmark['name'].replace('test_', '').replace('_performance', '')
                    mean = f\"{benchmark['stats']['mean']:.4f}\"
                    min_time = f\"{benchmark['stats']['min']:.4f}\"
                    max_time = f\"{benchmark['stats']['max']:.4f}\"
                    print(f'| {name} | {mean} | {min_time} | {max_time} |', file=sys.stderr)
                
                print('', file=sys.stderr)
                print('‚úÖ Performance benchmarks completed successfully!', file=sys.stderr)
                
            except Exception as e:
                print(f'‚ö†Ô∏è Could not parse benchmark results: {e}', file=sys.stderr)
            " 2>> $GITHUB_STEP_SUMMARY
          else
            echo "‚ö†Ô∏è No benchmark results found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: üí¨ Comment PR with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let comment = '## üéØ Performance Benchmark Results\n\n';
            
            try {
              if (fs.existsSync('benchmark_results.json')) {
                const data = JSON.parse(fs.readFileSync('benchmark_results.json', 'utf8'));
                
                comment += '| Algorithm | Mean Time (s) | Status |\n';
                comment += '|-----------|---------------|--------|\n';
                
                for (const benchmark of data.benchmarks || []) {
                  const name = benchmark.name.replace('test_', '').replace('_performance', '');
                  const mean = benchmark.stats.mean.toFixed(4);
                  const status = benchmark.stats.mean < 2.0 ? '‚úÖ Good' : '‚ö†Ô∏è Slow';
                  comment += `| ${name} | ${mean} | ${status} |\n`;
                }
                
                comment += '\nüìä Full benchmark results available in build artifacts.';
              } else {
                comment += '‚ö†Ô∏è Benchmark results not available - check workflow logs.';
              }
            } catch (error) {
              comment += `‚ö†Ô∏è Error parsing benchmark results: ${error.message}`;
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  comparative-analysis:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: üìä Performance Comparison
        run: |
          echo "## üìà Performance Comparison" >> $GITHUB_STEP_SUMMARY
          echo "Comparing PR performance against main branch..." >> $GITHUB_STEP_SUMMARY
          echo "This feature requires historical benchmark data to be fully functional." >> $GITHUB_STEP_SUMMARY
