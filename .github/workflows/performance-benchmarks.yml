name: 🚀 Performance Benchmarking

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance benchmarks every night at 2 AM
    - cron: '0 2 * * *'

env:
  BENCHMARK_ITERATIONS: 3
  BENCHMARK_TIMEOUT: 300

jobs:
  performance-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: routeforce_bench
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: 🐍 Set up Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: 📦 Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-benchmark-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-benchmark-
            ${{ runner.os }}-pip-

      - name: ⚙️ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark memory-profiler psutil

      - name: 🏃‍♂️ Run Route Optimization Benchmarks
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/routeforce_bench
          REDIS_URL: redis://localhost:6379/0
          FLASK_ENV: benchmarking
          PYTHONPATH: ${{ github.workspace }}
        run: |
          # Ensure test directory exists
          mkdir -p tests/benchmarks
          
          # Run the benchmarks
          pytest tests/benchmarks/test_performance.py \
            --benchmark-only \
            --benchmark-sort=mean \
            --benchmark-json=benchmark_results.json \
            --tb=short \
            -v || echo "⚠️ Some benchmarks may have failed, but continuing..."

      - name: 📊 Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            benchmark_results.json
          retention-days: 30
          if-no-files-found: warn

      - name: 📈 Performance Report
        if: always()
        run: |
          echo "## 🎯 Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          
          if [ -f benchmark_results.json ]; then
            echo "✅ Benchmark results found, generating report..." >> $GITHUB_STEP_SUMMARY
            python3 scripts/generate_benchmark_report.py >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ No benchmark results file found (benchmark_results.json)" >> $GITHUB_STEP_SUMMARY
            echo "This may indicate that benchmarks failed to run or complete." >> $GITHUB_STEP_SUMMARY
            
            # Check if pytest ran at all
            if [ -f pytest.log ]; then
              echo "� Pytest log found, showing last 10 lines:" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              tail -10 pytest.log >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
            fi
          fi

      - name: 💬 Comment PR with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let comment = '## 🎯 Performance Benchmark Results\n\n';
            
            try {
              if (fs.existsSync('benchmark_results.json')) {
                const data = JSON.parse(fs.readFileSync('benchmark_results.json', 'utf8'));
                
                comment += '| Algorithm | Mean Time (s) | Status |\n';
                comment += '|-----------|---------------|--------|\n';
                
                for (const benchmark of data.benchmarks || []) {
                  const name = benchmark.name.replace('test_', '').replace('_performance', '');
                  const mean = benchmark.stats.mean.toFixed(4);
                  const status = benchmark.stats.mean < 2.0 ? '✅ Good' : '⚠️ Slow';
                  comment += `| ${name} | ${mean} | ${status} |\n`;
                }
                
                comment += '\n📊 Full benchmark results available in build artifacts.';
              } else {
                comment += '⚠️ Benchmark results not available - check workflow logs.';
              }
            } catch (error) {
              comment += `⚠️ Error parsing benchmark results: ${error.message}`;
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  comparative-analysis:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: 📊 Performance Comparison
        run: |
          echo "## 📈 Performance Comparison" >> $GITHUB_STEP_SUMMARY
          echo "Comparing PR performance against main branch..." >> $GITHUB_STEP_SUMMARY
          echo "This feature requires historical benchmark data to be fully functional." >> $GITHUB_STEP_SUMMARY
